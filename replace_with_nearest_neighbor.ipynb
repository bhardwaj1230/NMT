{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nearest_neighbor_replace",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhardwaj1230/NMT/blob/master/replace_with_nearest_neighbor.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiBiGnnS1CCW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#!/usr/bin/env python\n",
        "# coding: utf-8\n",
        "\n",
        "import re\n",
        "import os\n",
        "import codecs\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "import pickle\n",
        "from pyfasttext import FastText as FT\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "import random\n",
        "import math\n",
        "import string\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ippfrm7K1K1Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.en.300.bin.gz\n",
        "# wget https://dl.fbaipublicfiles.com/fasttext/vectors-crawl/cc.fr.300.bin.gz\n",
        "    \n",
        "print('Loading English fastText pretrained model')\n",
        "\n",
        "#pretrained model:\n",
        "model_en = FT('/data/rali5/sans-bkp/shiven/classification/data/cc.en.300.bin')\n",
        "\n",
        "\n",
        "print('\\n\\n Reading europarl from directory')\n",
        "train_ep_en =[]\n",
        "\n",
        "en = '/data/rali5/sans-bkp/shiven/classification/data/train/train_en'\n",
        "# encoding=\"utf8\", errors='ignore'\n",
        "\n",
        "with open(en) as f:\n",
        "    eng = [line.strip('\\n') for line in tqdm(f)]\n",
        "    eng = [nltk.word_tokenize(line) for line in tqdm(eng[0:1000000])]\n",
        "    train_ep_en.append(eng)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W4llaPOI1B_X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Cleaning data for unique words')\n",
        "\n",
        "from operator import itemgetter  \n",
        "import random\n",
        "\n",
        "print('\\n\\n Cleaning')\n",
        "\n",
        "clean_en =[]\n",
        "for line in tqdm(train_ep_en[0]):\n",
        "    words = [word for word in line if word.isalpha()]\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    words = [w for w in words if not w in stop_words]\n",
        "    clean_en.append(words)\n",
        "\n",
        "flat_list = [item for sublist in clean_en for item in sublist]\n",
        "\n",
        "print('\\n\\n Calculating Zipf-Law')\n",
        "frequency= {}\n",
        "\n",
        "for word in tqdm(flat_list):\n",
        "    count = frequency.get(word,0)\n",
        "    frequency[word] = count + 1\n",
        "     \n",
        "zipf_data = {}\n",
        "\n",
        "for key, value in reversed(sorted(frequency.items(), key = itemgetter(1))):\n",
        "    zipf_data[key] = value\n",
        "\n",
        "print('\\n\\n Size of Vocab: ', len(zipf_data))\n",
        "\n",
        "'''\n",
        "freq_data = {key: value for key, value in zipf_data.items() if value > 100 }\n",
        "\n",
        "print('\\n\\n Number of words with frequency > 100: ', len(freq_data))\n",
        "\n",
        "words_to_nn = random.choices(list(freq_data.keys()), k = round(len(freq_data) * 0.3 ))\n",
        "\n",
        "print('\\n\\n Number of words to be replaced: ', len(words_to_nn))\n",
        "'''\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVs8BH8A1B6b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print('\\n\\n Started finding NN')\n",
        "\n",
        "nn_dict = {}\n",
        "for word in tqdm(list(zipf_data.keys())):\n",
        "    replaced_words = model_en.nearest_neighbors(word, k=5)\n",
        "    nn_dict[word] = replaced_words\n",
        "\n",
        "print('\\n\\n Writing words to directory')\n",
        "\n",
        "with open('/u/bhardwas/en_fr_data/nn_dict_en', 'wb') as fp:\n",
        "    pickle.dump(nn_dict, fp)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGyogjf6yVM0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('\\n\\n Reading English europarl from directory')\n",
        "train_ep_en =[]\n",
        "\n",
        "en = '/u/bhardwas/en_fr_data/europarl/fr-en/europarl-v7.fr-en.en'\n",
        "# encoding=\"utf8\", errors='ignore'\n",
        "\n",
        "with open(en) as f:\n",
        "    eng = [ re.sub(r'[^\\w\\s]', '', line).strip(' \\n').lower() for idx, line in tqdm(enumerate(f))]\n",
        "    eng = [nltk.word_tokenize(line) for line in tqdm(eng)]\n",
        "    train_ep_en.append(eng)\n",
        "\n",
        "\n",
        "print (\"\\n\\n Reading English Dictionary\")\n",
        "\n",
        "with open ('/u/bhardwas/en_fr_data/nn_dict_en', 'rb') as fp:\n",
        "    dict_en = pickle.load(fp)\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oxhcQf57yXXb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import string\n",
        "print('\\n\\n Start replacing with Nearest Neighbors for Englsih data')\n",
        "\n",
        "print (\"Random number with seed 101\")\n",
        "random.seed( 101 )\n",
        "\n",
        "neg_data_en = []\n",
        "for line in tqdm(train_ep_en[0]):\n",
        "    \n",
        "    big_word = [word for word in line if len(word) > 4]\n",
        "    sz = round(len(big_word) * 1) #100% replacement\n",
        "    \n",
        "    rand = random.sample(big_word, sz)\n",
        "    \n",
        "    replaced_words = []\n",
        "    for word_replace in rand:\n",
        "        if word_replace in dict_en:\n",
        "            test_word = random.choices(dict_en[word_replace],k=2)\n",
        "            test_word = list(test_word)\n",
        "            \n",
        "            if len(test_word[0][0]) > 3*len(word_replace):\n",
        "                replaced_words.append((word_replace,'1'))\n",
        "                \n",
        "            elif word_replace == test_word[0][0].lower():\n",
        "                replaced_words.append(test_word[1])\n",
        "            \n",
        "            elif '.' in test_word[0][0]:\n",
        "                if '.' in test_word[1][0]:\n",
        "                    replaced_words.append((word_replace,'1'))\n",
        "                else:\n",
        "                    replaced_words.append(test_word[1])\n",
        "            \n",
        "            else:\n",
        "                replaced_words.append(test_word[0])\n",
        "                \n",
        "        else :\n",
        "            replaced_words.append((word_replace,'1'))\n",
        "            \n",
        "    #print('Old word :', rand,'\\n','New Words :',  replaced_words,'\\n\\n')\n",
        "    \n",
        "    line = \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in line]).strip()\n",
        "    \n",
        "    for old, nn in zip(rand, replaced_words):\n",
        "        #print('Old word :', old,'\\n','New Words :',  nn,'\\n\\n')\n",
        "        line = line.replace(old, nn[0])\n",
        "\n",
        "    neg_data_en.append(line)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hlCS8bjbybhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "print('\\n\\n Writing English Negative Samples to directory')\n",
        "\n",
        "with open('/u/bhardwas/en_fr_data/neg_data_fr_en_en', 'wb') as fp:\n",
        "    pickle.dump(neg_data_en, fp)\n",
        "\n",
        "train_ep_en = 0\n",
        "neg_data_en = 0\n",
        "dict_en = 0\n",
        "\n",
        "\n",
        "\n",
        "print('\\n\\n Reading French europarl from directory')\n",
        "train_ep_fr =[]\n",
        "\n",
        "fr = '/u/bhardwas/en_fr_data/europarl/fr-en/europarl-v7.fr-en.fr'\n",
        "# encoding=\"utf8\", errors='ignore'\n",
        "\n",
        "with open(fr) as f:\n",
        "    frc = [ re.sub(r'[^\\w\\s]', '', line).strip(' \\n').lower() for idx, line in tqdm(enumerate(f))]\n",
        "    frc = [nltk.word_tokenize(line) for line in tqdm(frc)]\n",
        "    train_ep_fr.append(frc)\n",
        "\n",
        "print (\"\\n\\n Reading French Dictionary\")\n",
        "\n",
        "with open ('/u/bhardwas/en_fr_data/nn_dict_fr', 'rb') as fp:\n",
        "    dict_fr = pickle.load(fp)\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sGwF22iqye7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "import string\n",
        "print('\\n\\n Start replacing with Nearest Neighbors for French data')\n",
        "\n",
        "print (\"Random number with seed 101\")\n",
        "random.seed( 101 )\n",
        "\n",
        "neg_data_fr = []\n",
        "for line in tqdm(train_ep_fr[0]):\n",
        "    \n",
        "    big_word = [word for word in line if len(word) > 4]\n",
        "    sz = round(len(big_word) * 1) #100% replacement\n",
        "    \n",
        "    rand = random.sample(big_word, sz)\n",
        "    \n",
        "    replaced_words = []\n",
        "    for word_replace in rand:\n",
        "        if word_replace in dict_fr:\n",
        "            test_word = random.choices(dict_en[word_replace],k=2)\n",
        "            test_word = list(test_word)\n",
        "            \n",
        "            if len(test_word[0][0]) > 3*len(word_replace):\n",
        "                replaced_words.append((word_replace,'1'))\n",
        "                \n",
        "            elif word_replace == test_word[0][0].lower():\n",
        "                replaced_words.append(test_word[1])\n",
        "            \n",
        "            elif '.' in test_word[0][0]:\n",
        "                if '.' in test_word[1][0]:\n",
        "                    replaced_words.append((word_replace,'1'))\n",
        "                else:\n",
        "                    replaced_words.append(test_word[1])\n",
        "            \n",
        "            else:\n",
        "                replaced_words.append(test_word[0])\n",
        "                \n",
        "        else :\n",
        "            replaced_words.append((word_replace,'1'))\n",
        "            \n",
        "    line = \"\".join([\" \"+i if not i.startswith(\"'\") and i not in string.punctuation else i for i in line]).strip()\n",
        "    \n",
        "    for old, nn in zip(rand, replaced_words):\n",
        "        #print('Old word :', old,'\\n','New Words :',  nn,'\\n\\n')\n",
        "        line = line.replace(old, nn[0])\n",
        "\n",
        "    neg_data_fr.append(line)\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}